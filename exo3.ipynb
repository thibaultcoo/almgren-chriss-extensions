{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d043ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "addf5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman_IS_2D(lamb, sigma1, sigma2, eta1, eta2, rho, nb_stocks1, nb_stocks2, nb_periods):\n",
    "    # Initialize matrices to store rewards and policies for each stock count and period\n",
    "    reward = np.zeros((nb_stocks1 + 1, nb_stocks2 + 1, nb_periods))\n",
    "    policy = np.zeros((nb_stocks1 + 1, nb_stocks2 + 1, nb_periods, 2))\n",
    "    \n",
    "    # Iterate through each period in reverse order\n",
    "    for period in range(nb_periods-1, -1, -1):\n",
    "        # Iterate through each possible number of stocks for both dimensions\n",
    "        for stock1 in range(nb_stocks1 + 1):\n",
    "            for stock2 in range(nb_stocks2 + 1):\n",
    "                psi1 = stock1\n",
    "                psi2 = stock2\n",
    "                x1 = nb_stocks1 - psi1\n",
    "                x2 = nb_stocks2 - psi2\n",
    "\n",
    "                if psi1 == nb_stocks1 and psi2 == nb_stocks2:\n",
    "                    reward[psi1, psi2, period] = 0\n",
    "                    policy[psi1, psi2, period] = (0, 0)\n",
    "                else:\n",
    "                    if period == nb_periods - 1:\n",
    "                        reward[psi1, psi2, period] = np.inf\n",
    "                        policy[psi1, psi2, period] = (np.inf, np.inf)\n",
    "                    elif period == nb_periods - 2:\n",
    "                        reward[psi1, psi2, period] = -1 * lamb * ((sigma1 ** 2) * (x1 ** 2) + (sigma2 ** 2) * (x2 ** 2))\n",
    "                        reward[psi1, psi2, period] -= 2 * lamb * sigma1 * sigma2 * x1 * x2 * rho\n",
    "                        reward[psi1, psi2, period] -= eta1 * (psi1 ** 2) + eta2 * (psi2 ** 2)\n",
    "                        policy[psi1, psi2, period] = (x1, x2)\n",
    "                    else:\n",
    "                        # Calculate potential rewards for all possible actions\n",
    "                        best_reward = -np.inf\n",
    "                        best_policy = (0, 0)\n",
    "                        for i in range(x1 + 1):\n",
    "                            for j in range(x2 + 1):\n",
    "                                potential_reward = -1 * lamb * ((sigma1 ** 2) * (x1 ** 2) + (sigma2 ** 2) * (x2 ** 2))\n",
    "                                potential_reward -= 2 * lamb * sigma1 * sigma2 * x1 * x2 * rho\n",
    "                                potential_reward -= eta1 * (psi1 ** 2) + eta2 * (psi2 ** 2)\n",
    "                                if period < nb_periods - 1:\n",
    "                                    potential_reward += reward[psi1 + i, psi2 + j, period + 1]\n",
    "                                if potential_reward > best_reward:\n",
    "                                    best_reward = potential_reward\n",
    "                                    best_policy = (i, j)\n",
    "                        reward[psi1, psi2, period] = best_reward\n",
    "                        policy[psi1, psi2, period] = best_policy\n",
    "\n",
    "    # Determine the optimal trajectory of stock holdings over time\n",
    "    trajectory = [(nb_stocks1, nb_stocks2)]\n",
    "    current = (nb_stocks1, nb_stocks2)\n",
    "    \n",
    "    for period in range(1, nb_periods):\n",
    "        psi1, psi2 = policy[current[0], current[1], period - 1, 0], policy[current[0], current[1], period - 1, 1]\n",
    "        new_stock1 = current[0] - psi1\n",
    "        new_stock2 = current[1] - psi2\n",
    "        trajectory.append((new_stock1, new_stock2))\n",
    "        current = (new_stock1, new_stock2)\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "df0b3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_1 = 15\n",
    "x0_2 = 0\n",
    "\n",
    "sigma_1 = 0.20\n",
    "sigma_2 = 0.20\n",
    "rho = 0.9\n",
    "\n",
    "steps = 75\n",
    "\n",
    "eta_1 = 5 * 1e-3\n",
    "eta_2 = 5 * 1e-1\n",
    "lambdas = [5e-4, 1e-3, 1e-2 , 1e-1, 1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8f8202c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = {'t':[],'x_t':[],'lambda':[]}\n",
    "for lamb_value in lambdas:\n",
    "    traj = Bellman_IS_2D(lamb_value, sigma_1, sigma_2, eta_1, eta_2, rho, x0_1, x0_2, steps)\n",
    "    tab['t'].extend(range(len(traj)))\n",
    "    tab['x_t'].extend(traj)\n",
    "    tab['lambda'].extend([str(lamb_value) for i in range(len(traj))])\n",
    "\n",
    "df = pd.DataFrame(tab)\n",
    "fig = px.line(df, x='t', y='x_t', color='lambda', template='plotly_white', \n",
    "              labels={\"x_t\": \"Remaining Inventory\", \"t\": \"Time\", \"lambda\": \"Market Impact\"})\n",
    "\n",
    "fig.update_traces(hoverinfo='skip')\n",
    "fig.update_traces(hovertemplate=None)\n",
    "fig.update_layout(width=900, height=300)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
